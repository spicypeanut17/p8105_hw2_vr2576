P8105 Homework 2
================
Vaiju Raja (vr2576)
2024-09-27

## Problem 1: NYC Transit

``` r
# Load the CSV file
transit_data <- read_csv("NYC_Transit_Subway_Entrance_And_Exit_Data.csv")
```

    ## Rows: 1868 Columns: 32
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (22): Division, Line, Station Name, Route1, Route2, Route3, Route4, Rout...
    ## dbl  (8): Station Latitude, Station Longitude, Route8, Route9, Route10, Rout...
    ## lgl  (2): ADA, Free Crossover
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
# Clean dataset
transit <- transit_data |>
  janitor::clean_names() |>
  select(line, station_name, station_latitude, station_longitude,
    route1, route2, route3, route4, route5, route6, route7, 
    route8, route9, route10, route11, entry, vending, entrance_type, ada) |>
  unite("routes", starts_with("route"), na.rm = TRUE, remove = TRUE, sep = ",") |>
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE))

head(transit)
```

    ## # A tibble: 6 × 9
    ##   line     station_name station_latitude station_longitude routes entry vending
    ##   <chr>    <chr>                   <dbl>             <dbl> <chr>  <lgl> <chr>  
    ## 1 4 Avenue 25th St                  40.7             -74.0 R      TRUE  YES    
    ## 2 4 Avenue 25th St                  40.7             -74.0 R      TRUE  YES    
    ## 3 4 Avenue 36th St                  40.7             -74.0 N,R    TRUE  YES    
    ## 4 4 Avenue 36th St                  40.7             -74.0 N,R    TRUE  YES    
    ## 5 4 Avenue 36th St                  40.7             -74.0 N,R    TRUE  YES    
    ## 6 4 Avenue 45th St                  40.6             -74.0 R      TRUE  YES    
    ## # ℹ 2 more variables: entrance_type <chr>, ada <lgl>

``` r
n_rows <- nrow(transit)
n_cols <- ncol(transit)


# How many distinct stations are there?
distinct_stations <- transit |>
  distinct(station_name, line) |>
  nrow()

distinct_stations #465
```

    ## [1] 465

``` r
# How many stations are ADA compliant?
ada_compliant <- transit |>
  filter(ada == "TRUE") |>
  distinct(station_name, line) |>
  nrow()

ada_compliant #84
```

    ## [1] 84

``` r
# What proportion of station entrances/exits without vending allow entrance?
vending_proportion <- transit |>
  filter(vending == "NO") |>
  summarize(proportion = mean(entry)) |>
  pull(proportion) * 100 

vending_proportion #37.7%
```

    ## [1] 37.70492

``` r
# Reformat data so that route number and route name are distinct variables
transit_long <- transit |>
  separate_rows(routes, sep = ",") |>
  filter(routes != "")


# How many distinct stations serve the A train? 
a_train_stations <- transit_long |>
  filter(routes == "A") |>
  distinct(station_name, line) |>
  nrow()

a_train_stations #60
```

    ## [1] 60

``` r
# Of the stations that serve the A train, how many are ADA compliant?
ada_compliant_a_train_stations <- transit_long |>
  filter(routes == "A" & ada == "TRUE") |>
  distinct(station_name, line) |>
  nrow()

ada_compliant_a_train_stations #17
```

    ## [1] 17

The dataset includes information on various subway stations and their
attributes, such as line, station name, geographical coordinates, routes
served, entrance type, vending machines, and ADA compliance. After
cleaning the data by selecting relevant variables, converting the entry
variable to a logical type, and reformatting routes, the resulting
dataset has 1868 rows and 9 columns.

There are a total of 465 distinct subway stations when considering both
station names and lines. Of these, 84 stations are ADA-compliant. For
station entrances and exits without vending machines, 37.704918% allow
entry. Additionally, 60 distinct stations serve the A train, and of
those, 17 are ADA compliant.

## Problem 2: Mr. Trash Wheel

``` r
# Import and clean Mr Trash Wheel data
mr_trash_wheel <- read_excel(
  path = "202409 Trash Wheel Collection Data.xlsx",
  sheet = "Mr. Trash Wheel",
  skip = 1) |> 
  clean_names() |>
  filter(!is.na(dumpster)) |>  # Omit rows without dumpster-specific data
  select(-starts_with("x")) |>  # Two new columns being created - drop these columns
  mutate(
    sports_balls = as.integer(round(sports_balls)),  # Round and convert sports balls
    trash_wheel = "Mr. Trash Wheel",  # Add a column to track which trash wheel
    year = as.numeric(year)  # Ensure the year column is of the same type
  )
```

    ## New names:
    ## • `` -> `...15`
    ## • `` -> `...16`

``` r
# Import and clean Professor Trash Wheel data
professor_trash_wheel <- read_excel(
  path = "202409 Trash Wheel Collection Data.xlsx",
  sheet = "Professor Trash Wheel",
  skip = 1) |> 
  clean_names() |>
  filter(!is.na(dumpster)) |>  # Omit rows without dumpster-specific data
  select(-starts_with("x")) |>  # Two new columns being created - drop these columns
  mutate(
    trash_wheel = "Professor Trash Wheel",  # Add a column to track which trash wheel
    year = as.numeric(year)  # Ensure the year column is of the same type
  )


# Import and clean Gwynnda data
gwynnda <- read_excel(
  path = "202409 Trash Wheel Collection Data.xlsx",
  sheet = "Gwynnda Trash Wheel",
  skip = 1) |> 
  clean_names() |>
  filter(!is.na(dumpster)) |>  # Omit rows without dumpster-specific data
  select(-starts_with("x")) |>  # Two new columns being created - drop these columns
  mutate(
    trash_wheel = "Gwynnda",  # Add a column to track which trash wheel
    year = as.numeric(year)  # Ensure the year column is of the same type
  )


# Combine the datasets
combined_trash <- bind_rows(mr_trash_wheel, professor_trash_wheel, gwynnda)
combined_trash
```

    ## # A tibble: 1,033 × 15
    ##    dumpster month  year date                weight_tons volume_cubic_yards
    ##       <dbl> <chr> <dbl> <dttm>                    <dbl>              <dbl>
    ##  1        1 May    2014 2014-05-16 00:00:00        4.31                 18
    ##  2        2 May    2014 2014-05-16 00:00:00        2.74                 13
    ##  3        3 May    2014 2014-05-16 00:00:00        3.45                 15
    ##  4        4 May    2014 2014-05-17 00:00:00        3.1                  15
    ##  5        5 May    2014 2014-05-17 00:00:00        4.06                 18
    ##  6        6 May    2014 2014-05-20 00:00:00        2.71                 13
    ##  7        7 May    2014 2014-05-21 00:00:00        1.91                  8
    ##  8        8 May    2014 2014-05-28 00:00:00        3.7                  16
    ##  9        9 June   2014 2014-06-05 00:00:00        2.52                 14
    ## 10       10 June   2014 2014-06-11 00:00:00        3.76                 18
    ## # ℹ 1,023 more rows
    ## # ℹ 9 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, plastic_bags <dbl>,
    ## #   wrappers <dbl>, sports_balls <int>, homes_powered <dbl>, trash_wheel <chr>

``` r
# Example of calculating total weight collected by Professor Trash Wheel
total_weight_prof_trash_wheel <- combined_trash |> 
  filter(trash_wheel == "Professor Trash Wheel") |> 
  summarize(total_weight = sum(weight_tons, na.rm = TRUE))

total_weight_prof_trash_wheel #216
```

    ## # A tibble: 1 × 1
    ##   total_weight
    ##          <dbl>
    ## 1         247.

``` r
# Example of calculating total number of cigarette butts collected by Gwynnda in June 2022
total_cig_butts_june_2022 <- combined_trash |> 
  filter(trash_wheel == "Gwynnda", month == "July", year == 2022) |> 
  summarize(total_cigarette_butts = sum(cigarette_butts, na.rm = TRUE))

total_cig_butts_june_2022 #31090
```

    ## # A tibble: 1 × 1
    ##   total_cigarette_butts
    ##                   <dbl>
    ## 1                 31090

``` r
head(combined_trash)
```

    ## # A tibble: 6 × 15
    ##   dumpster month  year date                weight_tons volume_cubic_yards
    ##      <dbl> <chr> <dbl> <dttm>                    <dbl>              <dbl>
    ## 1        1 May    2014 2014-05-16 00:00:00        4.31                 18
    ## 2        2 May    2014 2014-05-16 00:00:00        2.74                 13
    ## 3        3 May    2014 2014-05-16 00:00:00        3.45                 15
    ## 4        4 May    2014 2014-05-17 00:00:00        3.1                  15
    ## 5        5 May    2014 2014-05-17 00:00:00        4.06                 18
    ## 6        6 May    2014 2014-05-20 00:00:00        2.71                 13
    ## # ℹ 9 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, plastic_bags <dbl>,
    ## #   wrappers <dbl>, sports_balls <int>, homes_powered <dbl>, trash_wheel <chr>

The dataset includes information on the trash collected by various trash
wheels, including Mr. Trash Wheel, Professor Trash Wheel, and Gwynnda.
After cleaning and organizing the data, the resulting dataset contains
1033 observations with key variables such as the dumpster count, weight
of trash collected, and specific items like plastic bottles and
cigarette butts. In total, Professor Trash Wheel collected 246.74 tons
of trash. Additionally, Gwynnda collected 3.109^{4} cigarette butts in
June 2022.

## Problem 3: The Great British Baking Off

``` r
# Bakers Dataset
bakers <- read_csv("gbb_datasets/bakers.csv") |>
  clean_names() |>
  rename(baker = baker_name)
```

    ## Rows: 120 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker Name, Baker Occupation, Hometown
    ## dbl (2): Series, Baker Age
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
str(bakers)
```

    ## spc_tbl_ [120 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)
    ##  $ baker           : chr [1:120] "Ali Imdad" "Alice Fevronia" "Alvin Magallanes" "Amelia LeBruin" ...
    ##  $ series          : num [1:120] 4 10 6 10 7 1 9 4 2 7 ...
    ##  $ baker_age       : num [1:120] 25 28 37 24 25 30 30 31 31 23 ...
    ##  $ baker_occupation: chr [1:120] "Charity worker" "Geography teacher" "Nurse" "Fashion designer" ...
    ##  $ hometown        : chr [1:120] "Saltley, Birmingham" "Essex" "Bracknell, Berkshire" "Halifax" ...
    ##  - attr(*, "spec")=
    ##   .. cols(
    ##   ..   `Baker Name` = col_character(),
    ##   ..   Series = col_double(),
    ##   ..   `Baker Age` = col_double(),
    ##   ..   `Baker Occupation` = col_character(),
    ##   ..   Hometown = col_character()
    ##   .. )
    ##  - attr(*, "problems")=<externalptr>

``` r
# Extract the last name from the baker's full name
bakers <- bakers |> 
  mutate(baker_last_name = word(baker, 2), baker = word(baker, 1))


# Bakes Dataset
bakes <- read_csv("gbb_datasets/bakes.csv") |>
  clean_names() 
```

    ## Rows: 548 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker, Signature Bake, Show Stopper
    ## dbl (2): Series, Episode
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
str(bakes)
```

    ## spc_tbl_ [548 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)
    ##  $ series        : num [1:548] 1 1 1 1 1 1 1 1 1 1 ...
    ##  $ episode       : num [1:548] 1 1 1 1 1 1 1 1 1 1 ...
    ##  $ baker         : chr [1:548] "Annetha" "David" "Edd" "Jasminder" ...
    ##  $ signature_bake: chr [1:548] "Light Jamaican Black Cakewith Strawberries and Cream" "Chocolate Orange Cake" "Caramel Cinnamon and Banana Cake" "Fresh Mango and Passion Fruit Hummingbird Cake" ...
    ##  $ show_stopper  : chr [1:548] "Red, White & Blue Chocolate Cake with Cigarellos, Fresh Fruit, and Cream" "Black Forest Floor Gateauxwith Moulded Chocolate Leaves, Fallen Fruitand Chocolate Mushrooms Moulded from eggs" "N/A" "N/A" ...
    ##  - attr(*, "spec")=
    ##   .. cols(
    ##   ..   Series = col_double(),
    ##   ..   Episode = col_double(),
    ##   ..   Baker = col_character(),
    ##   ..   `Signature Bake` = col_character(),
    ##   ..   `Show Stopper` = col_character()
    ##   .. )
    ##  - attr(*, "problems")=<externalptr>

``` r
# Results Dataset
results <- read_csv("gbb_datasets/results.csv") |> 
  clean_names() |> 
  rename(series = x1, episode = x2, baker = x3, technical = x4, result = in_stayed_in_out_eliminated_star_baker_star_baker_winner_series_winner_runner_up_series_runner_up_wd_withdrew) |>  # Rename variables 
  filter(!is.na(result)) |>  # Remove rows with no result info
  slice(-1) |>  # Remove the first row
  mutate(series = as.numeric(series), episode = as.numeric(episode), technical = as.numeric(technical)) # Convert series, technical, & episode to numeric
```

    ## New names:
    ## Rows: 1138 Columns: 5
    ## ── Column specification
    ## ──────────────────────────────────────────────────────── Delimiter: "," chr
    ## (5): ...1, ...2, ...3, ...4, IN = stayed in; OUT = Eliminated; STAR BAKE...
    ## ℹ Use `spec()` to retrieve the full column specification for this data. ℹ
    ## Specify the column types or set `show_col_types = FALSE` to quiet this message.
    ## • `` -> `...1`
    ## • `` -> `...2`
    ## • `` -> `...3`
    ## • `` -> `...4`

``` r
str(results)
```

    ## tibble [710 × 5] (S3: tbl_df/tbl/data.frame)
    ##  $ series   : num [1:710] 1 1 1 1 1 1 1 1 1 1 ...
    ##  $ episode  : num [1:710] 1 1 1 1 1 1 1 1 1 1 ...
    ##  $ baker    : chr [1:710] "Annetha" "David" "Edd" "Jasminder" ...
    ##  $ technical: num [1:710] 2 3 1 NA 9 NA 8 NA 10 NA ...
    ##  $ result   : chr [1:710] "IN" "IN" "IN" "IN" ...

``` r
# Viewers Dataset
# Pivot the dataset to long format
viewers <- read_csv("gbb_datasets/viewers.csv") |> 
  clean_names() |> 
  pivot_longer(cols = starts_with("series"), names_to = "series", values_to = "viewers") |> 
  mutate(series = as.numeric(str_remove(series, "series_"))) # Convert series to numeric and clean the series name
```

    ## Rows: 10 Columns: 11
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (11): Episode, Series 1, Series 2, Series 3, Series 4, Series 5, Series ...
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
str(viewers)
```

    ## tibble [100 × 3] (S3: tbl_df/tbl/data.frame)
    ##  $ episode: num [1:100] 1 1 1 1 1 1 1 1 1 1 ...
    ##  $ series : num [1:100] 1 2 3 4 5 6 7 8 9 10 ...
    ##  $ viewers: num [1:100] 2.24 3.1 3.85 6.6 8.51 ...

``` r
# Merge Bakers and Bakes Datasets
bakers_bakes <- left_join(bakers, bakes, by = c("baker", "series"))

# Merge the Bakers and Bakes Dataset with the Results Dataset
bakers_bakes_results <- left_join(bakers_bakes, results, by = c("baker", "series", "episode"))

# Merge the Bakers, Bakes, & Results Dataset with the Viewers Dataset
final <- left_join(bakers_bakes_results, viewers, by = c("series", "episode"))


# Organize & export the final dataset
final <- final |> 
  select(series, episode, baker, baker_last_name, baker_age, baker_occupation, hometown, signature_bake, technical, show_stopper, result, viewers)
write_csv(final, "final_bake_off_data.csv")


# Star Baker & Winner Table for Seasons 5 to 10
star_bakers_winners <- final |>
  filter(series >= 5 & series <= 10, result %in% c("STAR BAKER", "WINNER")) |>
  select(series, episode, baker, result) |>
  arrange(series, episode)

knitr::kable(star_bakers_winners, caption = "Star Bakers and Winners (Seasons 5-10)")
```

| series | episode | baker     | result     |
|-------:|--------:|:----------|:-----------|
|      5 |       1 | Nancy     | STAR BAKER |
|      5 |       2 | Richard   | STAR BAKER |
|      5 |       3 | Luis      | STAR BAKER |
|      5 |       4 | Richard   | STAR BAKER |
|      5 |       5 | Kate      | STAR BAKER |
|      5 |       6 | Chetna    | STAR BAKER |
|      5 |       7 | Richard   | STAR BAKER |
|      5 |       8 | Richard   | STAR BAKER |
|      5 |       9 | Richard   | STAR BAKER |
|      5 |      10 | Nancy     | WINNER     |
|      6 |       1 | Marie     | STAR BAKER |
|      6 |       2 | Ian       | STAR BAKER |
|      6 |       3 | Ian       | STAR BAKER |
|      6 |       4 | Ian       | STAR BAKER |
|      6 |       5 | Nadiya    | STAR BAKER |
|      6 |       6 | Mat       | STAR BAKER |
|      6 |       7 | Tamal     | STAR BAKER |
|      6 |       8 | Nadiya    | STAR BAKER |
|      6 |       9 | Nadiya    | STAR BAKER |
|      6 |      10 | Nadiya    | WINNER     |
|      7 |       1 | Jane      | STAR BAKER |
|      7 |       2 | Candice   | STAR BAKER |
|      7 |       3 | Tom       | STAR BAKER |
|      7 |       4 | Benjamina | STAR BAKER |
|      7 |       5 | Candice   | STAR BAKER |
|      7 |       6 | Tom       | STAR BAKER |
|      7 |       7 | Andrew    | STAR BAKER |
|      7 |       8 | Candice   | STAR BAKER |
|      7 |       9 | Andrew    | STAR BAKER |
|      7 |      10 | Candice   | WINNER     |
|      8 |       1 | Steven    | STAR BAKER |
|      8 |       2 | Steven    | STAR BAKER |
|      8 |       3 | Julia     | STAR BAKER |
|      8 |       4 | Kate      | STAR BAKER |
|      8 |       5 | Sophie    | STAR BAKER |
|      8 |       6 | Liam      | STAR BAKER |
|      8 |       7 | Steven    | STAR BAKER |
|      8 |       8 | Stacey    | STAR BAKER |
|      8 |       9 | Sophie    | STAR BAKER |
|      8 |      10 | Sophie    | WINNER     |

Star Bakers and Winners (Seasons 5-10)

``` r
# Average Viewership in Seasons 1 and 5
avg_views_s1 <- final |> 
  filter(series == 1) |> 
  summarize(avg_views = mean(viewers, na.rm = TRUE))
avg_views_s5 <- final |> 
  filter(series == 5) |> 
  summarize(avg_views = mean(viewers, na.rm = TRUE))

avg_views_s1 #2.72
```

    ## # A tibble: 1 × 1
    ##   avg_views
    ##       <dbl>
    ## 1      2.72

``` r
avg_views_s5 #9.65
```

    ## # A tibble: 1 × 1
    ##   avg_views
    ##       <dbl>
    ## 1      9.65

**Data Cleaning Process**

The data cleaning process began by importing datasets for bakers, bakes,
results, and viewership. One challenge was that the baker names in the
bakers dataset contained both first and last names, while the bakes
dataset only used first names. To resolve this, I manually matched and
merged the datasets by creating a consistent baker identifier across
both datasets.

For the results dataset, I removed unnecessary rows (such as the first
row) and converted key variables like series and episode into consistent
formats to enable successful joins. The cleaned dataset allowed us to
merge baker details, bakes, and results into a single dataset, ensuring
that every episode’s baker performances were appropriately linked with
outcomes like elimination or star baker.

I encountered some type mismatches and many-to-many relationships
between the series columns across datasets, which were resolved by
joining on both baker, series, and episode to avoid duplicate data.
Additionally, the viewership dataset required conversion of columns from
wide to long format and renaming variables for consistency.

The final dataset combines key information about each baker (age,
occupation, hometown), their bakes (signature bakes, showstoppers), and
results (eliminations, star baker, series winner). The viewership
dataset now shows the viewership for each episode across all seasons.
The final dataset has 566 observations and includes key variables such
as “baker,” “series,” and “episode.” For instance, there are 107 unique
bakers across 10 seasons.

**Star Bakers and Winners**

Some winners displayed consistent performance by being star bakers
multiple times across episodes, which made them more predictable towards
the end of the season. Nadiya from Season 6 and Candice from Season 7
won multiple star baker titles and went on to win their respective
series, which was more expected. In a few seasons, star bakers did not
necessarily become the overall winner. For instance, Richard won star
baker the most in season 5 but Nancy won the entire season.

**Average Viewership**

The average viewership in Season 1 was 2.7158333 million people. The
average viewership in Season 5 was 9.6478767 million people.
